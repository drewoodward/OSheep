# O'Sheep ğŸ‘

https://o-sheep.vercel.app

**Your Personal Ollama Chat Interface via Tailscale**

A beautiful, privacy-focused web application that lets you chat with AI models running on your own Ollama instance, accessible anywhere via Tailscale Funnel.

## ğŸŒŸ Features

- ğŸ”’ **Privacy First**: Direct browser-to-Ollama connection, no third-party servers
- ğŸŒ **Remote Access**: Connect to your home/office Ollama via Tailscale Funnel
- ğŸ’¬ **Modern UI**: Clean, responsive chat interface with dark mode
- ğŸ¨ **Multi-Model**: Switch between available models on the fly
- ğŸ’¾ **Persistent**: Remembers your connection settings
- ğŸš€ **Serverless**: Deploy on Vercel for free

## ğŸ—ï¸ Project Structure

```
O'Sheep/
â”œâ”€â”€ frontend/           # Next.js web application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/       # Next.js App Router pages
â”‚   â”‚   â””â”€â”€ components/ # React components
â”‚   â”œâ”€â”€ README.md      # Frontend documentation
â”‚   â””â”€â”€ DEPLOYMENT.md  # Vercel deployment guide
â””â”€â”€ test/              # API testing scripts
    â””â”€â”€ api-test.py    # Python test script for Ollama API
```

## ğŸš€ Quick Start

### Prerequisites

1. **Ollama** installed and running ([ollama.ai](https://ollama.ai))
2. **Tailscale** with Funnel enabled ([tailscale.com](https://tailscale.com))
3. **Node.js 18+** for local development

### Setup Ollama with Tailscale

```bash
# Pull a model
ollama pull llama3.2

# Enable Tailscale Funnel (makes Ollama accessible remotely)
tailscale funnel 11434
```

Your Ollama is now accessible at: `https://your-machine.your-tailnet.ts.net`

### Run Frontend Locally

```bash
cd frontend
npm install
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) and enter your Tailscale Funnel URL.

### Deploy to Vercel

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new)

See [frontend/DEPLOYMENT.md](frontend/DEPLOYMENT.md) for detailed instructions.

## ğŸ§ª Testing

Test your Ollama instance before deploying:

```bash
cd test
python api-test.py
```

## ğŸ”§ Technology Stack

- **Frontend**: Next.js 14, React, TypeScript, Tailwind CSS
- **Backend**: None! Direct client-to-Ollama communication
- **Deployment**: Vercel (free tier)
- **Icons**: Lucide React

## ğŸ” Security

- All AI interactions happen between your browser and your Ollama instance
- No data passes through third-party servers
- Connection URL stored only in browser localStorage
- Secured by Tailscale's zero-trust network

## ğŸ“– Documentation

- [Frontend README](frontend/README.md) - Detailed usage guide
- [Deployment Guide](frontend/DEPLOYMENT.md) - Vercel deployment steps

## ğŸ¤ Contributing

Contributions welcome! Feel free to open issues or submit pull requests.

## ğŸ“ License

MIT

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) - Amazing local AI runtime
- [Tailscale](https://tailscale.com) - Secure remote access
- [Next.js](https://nextjs.org) - React framework
- [Vercel](https://vercel.com) - Hosting platform

---

Built with â¤ï¸ for privacy-conscious AI enthusiasts
